[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "silhouette_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "silhouette_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "silhouette_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "silhouette_samples",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "ListedColormap",
        "importPath": "matplotlib.colors",
        "description": "matplotlib.colors",
        "isExtraImport": true,
        "detail": "matplotlib.colors",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "BatchNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Flatten",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "BatchNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "tensorflow.keras.callbacks",
        "description": "tensorflow.keras.callbacks",
        "isExtraImport": true,
        "detail": "tensorflow.keras.callbacks",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "tensorflow.keras.callbacks",
        "description": "tensorflow.keras.callbacks",
        "isExtraImport": true,
        "detail": "tensorflow.keras.callbacks",
        "documentation": {}
    },
    {
        "label": "LearningRateScheduler",
        "importPath": "tensorflow.keras.callbacks",
        "description": "tensorflow.keras.callbacks",
        "isExtraImport": true,
        "detail": "tensorflow.keras.callbacks",
        "documentation": {}
    },
    {
        "label": "to_categorical",
        "importPath": "tensorflow.keras.utils",
        "description": "tensorflow.keras.utils",
        "isExtraImport": true,
        "detail": "tensorflow.keras.utils",
        "documentation": {}
    },
    {
        "label": "to_categorical",
        "importPath": "tensorflow.keras.utils",
        "description": "tensorflow.keras.utils",
        "isExtraImport": true,
        "detail": "tensorflow.keras.utils",
        "documentation": {}
    },
    {
        "label": "to_categorical",
        "importPath": "tensorflow.keras.utils",
        "description": "tensorflow.keras.utils",
        "isExtraImport": true,
        "detail": "tensorflow.keras.utils",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "mnist",
        "importPath": "tensorflow.keras.datasets",
        "description": "tensorflow.keras.datasets",
        "isExtraImport": true,
        "detail": "tensorflow.keras.datasets",
        "documentation": {}
    },
    {
        "label": "Adam",
        "importPath": "tensorflow.keras.optimizers",
        "description": "tensorflow.keras.optimizers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.optimizers",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "a = np.random.randint(1, 31, size=(25, 2))\n# b. 25 2-D random integer samples in the range of 45-75\nb = np.random.randint(45, 76, size=(25, 2))\n# c. 25 2-D random integer samples in the range of 150-200\nc = np.random.randint(150, 201, size=(25, 2))\n# Concatenate a, b, c to create the dataset\ndataset = np.concatenate((a, b, c), axis=0)\n# Step 2: Implement K-means clustering with the Silhouette method to find optimal k\nsilhouette_scores = []\nk_values = range(2, 11)",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "b = np.random.randint(45, 76, size=(25, 2))\n# c. 25 2-D random integer samples in the range of 150-200\nc = np.random.randint(150, 201, size=(25, 2))\n# Concatenate a, b, c to create the dataset\ndataset = np.concatenate((a, b, c), axis=0)\n# Step 2: Implement K-means clustering with the Silhouette method to find optimal k\nsilhouette_scores = []\nk_values = range(2, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "c = np.random.randint(150, 201, size=(25, 2))\n# Concatenate a, b, c to create the dataset\ndataset = np.concatenate((a, b, c), axis=0)\n# Step 2: Implement K-means clustering with the Silhouette method to find optimal k\nsilhouette_scores = []\nk_values = range(2, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(dataset)\n    score = silhouette_score(dataset, labels)",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "dataset = np.concatenate((a, b, c), axis=0)\n# Step 2: Implement K-means clustering with the Silhouette method to find optimal k\nsilhouette_scores = []\nk_values = range(2, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(dataset)\n    score = silhouette_score(dataset, labels)\n    silhouette_scores.append(score)\n# Finding the optimal k (with maximum silhouette score)",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "silhouette_scores",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "silhouette_scores = []\nk_values = range(2, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(dataset)\n    score = silhouette_score(dataset, labels)\n    silhouette_scores.append(score)\n# Finding the optimal k (with maximum silhouette score)\noptimal_k = k_values[np.argmax(silhouette_scores)]\nprint(f\"Optimal number of clusters (k): {optimal_k}\")",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "k_values",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "k_values = range(2, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(dataset)\n    score = silhouette_score(dataset, labels)\n    silhouette_scores.append(score)\n# Finding the optimal k (with maximum silhouette score)\noptimal_k = k_values[np.argmax(silhouette_scores)]\nprint(f\"Optimal number of clusters (k): {optimal_k}\")\n# Step 3: Apply K-means with the optimal k value",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "optimal_k",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "optimal_k = k_values[np.argmax(silhouette_scores)]\nprint(f\"Optimal number of clusters (k): {optimal_k}\")\n# Step 3: Apply K-means with the optimal k value\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nlabels = kmeans.fit_predict(dataset)\ncentroids = kmeans.cluster_centers_\n# Plot the clustered results\nplt.figure(figsize=(10, 6))\nplt.title(f\"K-Means Clustering with k={optimal_k}\", fontsize=14)\n# Create a discrete colormap with optimal_k colors",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "kmeans",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\nlabels = kmeans.fit_predict(dataset)\ncentroids = kmeans.cluster_centers_\n# Plot the clustered results\nplt.figure(figsize=(10, 6))\nplt.title(f\"K-Means Clustering with k={optimal_k}\", fontsize=14)\n# Create a discrete colormap with optimal_k colors\nbase_colors = plt.colormaps[\"tab10\"](np.linspace(0, 1, optimal_k))  # Updated color map access\ncolors = [base_colors[i] for i in range(optimal_k)]  # Assign colors to each cluster\n# Plot each cluster with a different color",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "labels = kmeans.fit_predict(dataset)\ncentroids = kmeans.cluster_centers_\n# Plot the clustered results\nplt.figure(figsize=(10, 6))\nplt.title(f\"K-Means Clustering with k={optimal_k}\", fontsize=14)\n# Create a discrete colormap with optimal_k colors\nbase_colors = plt.colormaps[\"tab10\"](np.linspace(0, 1, optimal_k))  # Updated color map access\ncolors = [base_colors[i] for i in range(optimal_k)]  # Assign colors to each cluster\n# Plot each cluster with a different color\nfor i in range(optimal_k):",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "centroids",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "centroids = kmeans.cluster_centers_\n# Plot the clustered results\nplt.figure(figsize=(10, 6))\nplt.title(f\"K-Means Clustering with k={optimal_k}\", fontsize=14)\n# Create a discrete colormap with optimal_k colors\nbase_colors = plt.colormaps[\"tab10\"](np.linspace(0, 1, optimal_k))  # Updated color map access\ncolors = [base_colors[i] for i in range(optimal_k)]  # Assign colors to each cluster\n# Plot each cluster with a different color\nfor i in range(optimal_k):\n    cluster_points = dataset[labels == i]",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "base_colors",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "base_colors = plt.colormaps[\"tab10\"](np.linspace(0, 1, optimal_k))  # Updated color map access\ncolors = [base_colors[i] for i in range(optimal_k)]  # Assign colors to each cluster\n# Plot each cluster with a different color\nfor i in range(optimal_k):\n    cluster_points = dataset[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, color=colors[i], label=f'Cluster {i + 1}')\n# Plot the centroids\nplt.scatter(centroids[:, 0], centroids[:, 1], s=200, color='red', marker='X', label='Centroids')\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "1",
        "description": "1",
        "peekOfCode": "colors = [base_colors[i] for i in range(optimal_k)]  # Assign colors to each cluster\n# Plot each cluster with a different color\nfor i in range(optimal_k):\n    cluster_points = dataset[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, color=colors[i], label=f'Cluster {i + 1}')\n# Plot the centroids\nplt.scatter(centroids[:, 0], centroids[:, 1], s=200, color='red', marker='X', label='Centroids')\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.legend()",
        "detail": "1",
        "documentation": {}
    },
    {
        "label": "x_train_full",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "x_train_full = x_train_full / 255.0  # Normalize pixel values to [0, 1]\nx_test = x_test / 255.0\n# Reshape for the MLP input (Flattening will be handled in the model)\nx_train_full = x_train_full.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n# One-hot encode labels\ny_train_full = to_categorical(y_train_full, 10)\ny_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "x_test",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "x_test = x_test / 255.0\n# Reshape for the MLP input (Flattening will be handled in the model)\nx_train_full = x_train_full.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n# One-hot encode labels\ny_train_full = to_categorical(y_train_full, 10)\ny_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)\n# Hyperparameters",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "x_train_full",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "x_train_full = x_train_full.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n# One-hot encode labels\ny_train_full = to_categorical(y_train_full, 10)\ny_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)\n# Hyperparameters\nnum_layers = 6\nlayer_sizes = [512, 256, 128, 64, 32, 16]",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "x_test",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "x_test = x_test.reshape(-1, 28, 28, 1)\n# One-hot encode labels\ny_train_full = to_categorical(y_train_full, 10)\ny_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)\n# Hyperparameters\nnum_layers = 6\nlayer_sizes = [512, 256, 128, 64, 32, 16]\nactivation_function = 'relu'",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "y_train_full",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "y_train_full = to_categorical(y_train_full, 10)\ny_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)\n# Hyperparameters\nnum_layers = 6\nlayer_sizes = [512, 256, 128, 64, 32, 16]\nactivation_function = 'relu'\ndropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "y_test",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "y_test = to_categorical(y_test, 10)\n# Step 3: Train-Test Split (85% Train, 15% Validation)\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.15, random_state=42)\n# Hyperparameters\nnum_layers = 6\nlayer_sizes = [512, 256, 128, 64, 32, 16]\nactivation_function = 'relu'\ndropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001\nbatch_size = 64",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "num_layers",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "num_layers = 6\nlayer_sizes = [512, 256, 128, 64, 32, 16]\nactivation_function = 'relu'\ndropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "layer_sizes",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "layer_sizes = [512, 256, 128, 64, 32, 16]\nactivation_function = 'relu'\ndropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "activation_function",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "activation_function = 'relu'\ndropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "dropout_rates",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "dropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "learning_rate",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "learning_rate = 0.001\nbatch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[0]),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "batch_size = 64\nepochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[0]),\n    Dense(layer_sizes[1], activation=activation_function),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "epochs = 25\npatience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[0]),\n    Dense(layer_sizes[1], activation=activation_function),\n    BatchNormalization(),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "patience",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "patience = 10\n# Step 4: Build the Improved MLP Model with 6 layers\nmodel = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[0]),\n    Dense(layer_sizes[1], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[1]),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "model = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(layer_sizes[0], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[0]),\n    Dense(layer_sizes[1], activation=activation_function),\n    BatchNormalization(),\n    Dropout(dropout_rates[1]),\n    Dense(layer_sizes[2], activation=activation_function),\n    BatchNormalization(),",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n# Print Hyperparameters\nprint(\"Hyperparameters:\")\nprint(f\"Number of Layers: {num_layers}\")\nprint(f\"Layer Sizes: {layer_sizes}\")\nprint(f\"Activation Function: {activation_function}\")\nprint(f\"Dropout Rates: {dropout_rates}\")\nprint(f\"Learning Rate: {learning_rate}\")\nprint(f\"Batch Size: {batch_size}\")",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "early_stopping",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n# Step 7: Train the model with increased epochs and early stopping\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    epochs=epochs,\n    batch_size=batch_size,\n    callbacks=[early_stopping],\n    verbose=1\n)",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "2",
        "description": "2",
        "peekOfCode": "history = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    epochs=epochs,\n    batch_size=batch_size,\n    callbacks=[early_stopping],\n    verbose=1\n)\n# Step 8: Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)",
        "detail": "2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "rithu",
        "description": "rithu",
        "peekOfCode": "def main():\n    # Clustering\n    np.random.seed(0)\n    a = np.random.randint(1, 31, (25, 2))\n    b = np.random.randint(45, 76, (25, 2))\n    c = np.random.randint(150, 201, (25, 2))\n    dataset = np.concatenate((a, b, c), axis=0)\n    silhouette_scores, k_values = [], range(2, 10)\n    for k in k_values:\n        kmeans = KMeans(n_clusters=k, random_state=0).fit(dataset)",
        "detail": "rithu",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "rithu",
        "description": "rithu",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow info/warning messages\ndef main():\n    # Clustering\n    np.random.seed(0)\n    a = np.random.randint(1, 31, (25, 2))\n    b = np.random.randint(45, 76, (25, 2))\n    c = np.random.randint(150, 201, (25, 2))\n    dataset = np.concatenate((a, b, c), axis=0)\n    silhouette_scores, k_values = [], range(2, 10)\n    for k in k_values:",
        "detail": "rithu",
        "documentation": {}
    },
    {
        "label": "perform_clustering",
        "kind": 2,
        "importPath": "S20220010170_A10",
        "description": "S20220010170_A10",
        "peekOfCode": "def perform_clustering():\n    a = np.random.randint(1, 31, size=(25, 2))\n    b = np.random.randint(45, 76, size=(25, 2))\n    c = np.random.randint(150, 201, size=(25, 2))\n    dataset = np.concatenate((a, b, c), axis=0)\n    silhouette_scores = []\n    k_values = range(2, 11)\n    for k in k_values:\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        labels = kmeans.fit_predict(dataset)",
        "detail": "S20220010170_A10",
        "documentation": {}
    },
    {
        "label": "perform_classification",
        "kind": 2,
        "importPath": "S20220010170_A10",
        "description": "S20220010170_A10",
        "peekOfCode": "def perform_classification():\n    layer_sizes = [512, 256, 128, 64, 32, 16]\n    activation_function = 'relu'\n    dropout_rates = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2]\n    learning_rate = 0.001\n    batch_size = 64\n    epochs = 25\n    patience = 10\n    (x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    x_train_full, x_test = x_train_full / 255.0, x_test / 255.0",
        "detail": "S20220010170_A10",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "S20220010170_A10",
        "description": "S20220010170_A10",
        "peekOfCode": "def main():\n    print(\"Performing K-means Clustering:\")\n    perform_clustering()\n    print(\"\\nPerforming MNIST Classification with MLP:\")\n    perform_classification()\nif __name__ == \"__main__\":\n    main()",
        "detail": "S20220010170_A10",
        "documentation": {}
    }
]